{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspaces/ACOS_BERT/src\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspaces/ACOS_BERT/src\n"
     ]
    }
   ],
   "source": [
    "%cd ../src\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [2, 7094, 1774, 89, 6783, 22102, 3538, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "text = 'This is an example'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('trituenhantaoio/bert-base-vietnamese-uncased')\n",
    "\n",
    "tokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 B-Target\n",
      "4 I-Target\n",
      "5 B-Opinion\n",
      "6 I-Opinion\n",
      "/workspaces/ACOS_BERT/data/ViRes/Train.txt\n",
      "/workspaces/ACOS_BERT/data/ViRes/Dev.txt\n",
      "/workspaces/ACOS_BERT/data/ViRes/Test.txt\n"
     ]
    }
   ],
   "source": [
    "from data_setup import *\n",
    "\n",
    "label_set = LabelSet(['Target', 'Opinion'])\n",
    "processor = DataProcessor('/workspaces/ACOS_BERT/data/ViRes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('google-bert/bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'nhân viên', 'start': 23, 'end': 32, 'label': 'Target'},\n",
       " {'text': 'null', 'start': -1, 'end': -1, 'label': 'Opinion'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = processor.train_examples[0]\n",
    "[example.target, example.opinion]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Testing...\n",
      "['food style_options positive', 'food style_options negative', 'food style_options neutral', 'food prices positive', 'food prices negative', 'food prices neutral', 'food quality positive', 'food quality negative', 'food quality neutral', 'drinks style_options positive', 'drinks style_options negative', 'drinks style_options neutral', 'drinks quality positive', 'drinks quality negative', 'drinks quality neutral', 'drinks prices positive', 'drinks prices negative', 'drinks prices neutral', 'restaurant general positive', 'restaurant general negative', 'restaurant general neutral', 'restaurant miscellaneous positive', 'restaurant miscellaneous negative', 'restaurant miscellaneous neutral', 'restaurant prices positive', 'restaurant prices negative', 'restaurant prices neutral', 'location general positive', 'location general negative', 'location general neutral', 'service general positive', 'service general negative', 'service general neutral', 'ambience general positive', 'ambience general negative', 'ambience general neutral']\n"
     ]
    }
   ],
   "source": [
    "!python utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[SEP]'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type_ids = tokenizer(example.text, 'food style_options positive')[0].type_ids\n",
    "ner_mask = [1 if type_id == 0 else 0 for type_id in type_ids]\n",
    "sep_idx = sum(ner_mask) - 1\n",
    "print(ner_mask)\n",
    "ner_mask[sep_idx] = 0\n",
    "print(ner_mask)\n",
    "tokenizer(example.text, 'food style_options positive')[0].tokens[sep_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-Target',\n",
       " 'I-Target',\n",
       " 'I-Target',\n",
       " 'I-Target',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "align_tokens_and_annotations_bio(tokenizer(example.text, padding='max_length', max_length=128)[0], [example.target, example.opinion])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'k', '##hong', 'gia', '##n', 'ho', '##i', 'chat', 'va', 'nh', '##an', 'vie', '##n', 'k', '##hi', '##en', 'k', '##ha', '##ch', 'de', 'chi', '##u', 'qu', '##a', 'chung', '[SEP]', 'food', 'style', '_', 'options', 'positive', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer(example.text, 'food style_options positive', padding='max_length', max_length=128)[0].tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "def get_ner_mask(tokenized: Encoding, max_seq_length: int):\n",
    "    segment_ids = tokenized.type_ids\n",
    "    sep_idx = segment_ids.index(1) - 1\n",
    "    ner_mask = [1] * sep_idx + [0] * (max_seq_length - sep_idx)\n",
    "\n",
    "    return ner_mask\n",
    "\n",
    "ner_mask = get_ner_mask(encoding, 128)\n",
    "print(ner_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[SEP]',\n",
       " 'food',\n",
       " 'style',\n",
       " '_',\n",
       " 'options',\n",
       " 'positive',\n",
       " '[SEP]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = [encoding.tokens[idx] for idx, ner_mask_id in enumerate(ner_mask) if ner_mask_id == 0]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tokenizer(example.text, 'food style_options positive', padding='max_length', max_length=128)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n"
     ]
    }
   ],
   "source": [
    "print(encoding.type_ids.index(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n"
     ]
    }
   ],
   "source": [
    "print(len(encoding.type_ids[:26]) + (128 - 26))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
